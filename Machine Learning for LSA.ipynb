{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "\n",
    "from arcpy.da import *\n",
    "from arcpy.sa import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys \n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.formula.api import ols\n",
    "import warnings\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate, cross_val_score,cross_val_predict, KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "Project_path = r'E:\\科研工作01\\学术论文\\01区域尺度滑坡易发性对比分析_英文\\代码开源\\Machine Learning for LSA\\Machine Learning for LSA'\n",
    "arcpy.env.workspace = os.path.join(Project_path, 'Machine Learning for LSA.gdb')\n",
    "factors_address = r'E:\\科研工作01\\学术论文\\01区域尺度滑坡易发性对比分析_英文\\数据库\\秦巴山区要素\\秦巴山区要素.gdb'\n",
    "tabel_address = r'E:\\科研工作01\\学术论文\\01区域尺度滑坡易发性对比分析_英文\\统计表格'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate factors' importance and collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert geographic feature to pd.DataFrame\n",
    "\n",
    "def CovertFeature2df(feature, fields):\n",
    "    feature_arr = arcpy.da.FeatureClassToNumPyArray(feature, fields)\n",
    "    return pd.DataFrame(feature_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geo-Environmental factor importance (SA) calculation function\n",
    "\n",
    "def calculate_important_factor(landslide_feature_df, all_feature_df, feature_field, bin_x = None, Continue = True):\n",
    "    \n",
    "    if Continue:\n",
    "        # If it is a continuous factor, calculate the maximum and minimum values of the factor\n",
    "        max_value = max(all_feature_df.loc[:, feature_field])\n",
    "        min_value = min(all_feature_df.loc[:, feature_field])\n",
    "    else:\n",
    "        # If it is a discrete factor, calculate the number of features\n",
    "        unique_feature = len(all_feature_df.loc[:, feature_field].unique())\n",
    "\n",
    "    # The name of the histogram horizontal axis\n",
    "    bin_name = feature_field\n",
    "    \n",
    "    if bin_x is None:\n",
    "        if Continue:\n",
    "            bin_x = np.linspace(min_value, max_value, 30)        \n",
    "        else:\n",
    "            bin_x = np.array(range(unique_feature))+1\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate histogram\n",
    "    h_land ,_ ,_ = plt.hist(landslide_feature_df.loc[:, feature_field],\n",
    "                             bins=bin_x)\n",
    "    \n",
    "    h_full,_,_ = plt.hist(all_feature_df.loc[:, feature_field],\n",
    "                              bins=bin_x)\n",
    "    \n",
    "    h_full[h_full == 0] = 1e-6 # avoid 0/0\n",
    "    h_land[h_land == 0] = 1e-8\n",
    "    \n",
    "    # Bayes formula\n",
    "    \n",
    "    P_R = h_full/h_full.sum()\n",
    "    P_L = h_land.sum()/h_full.sum()\n",
    "    P_R_L = h_land/h_land.sum()\n",
    "    P_L_R = h_land/h_full\n",
    "    P_L_R[P_L_R > 1] = 1\n",
    "    bin_width = 1/len(bin_x[0:-1])\n",
    "    \n",
    "    # calculate the importance\n",
    "    Ent_func = lambda Px, Py: -(Px/(Px+Py)*np.log2(Px/(Px+Py))+Py/(Px+Py)*np.log2(Py/(Px+Py)))\n",
    "    Vector_Ent_func = np.vectorize(Ent_func)\n",
    "    \n",
    "    Ent_feature = Vector_Ent_func(h_land, h_full)  \n",
    "    Gain_temp = Ent_func(h_land.sum(), h_full.sum())- np.sum((h_land+h_full)/np.sum(h_land+h_full) * Ent_feature)\n",
    "    \n",
    "    JS_divergence_func = lambda x, y: 0.5*np.sum(x*np.log2(2*x/(x+y)))+0.5*np.sum(y*np.log2(2*y/(x+y)))\n",
    "    P_R_L[P_R_L == 0]=1e-8 \n",
    "    JS_temp = JS_divergence_func(P_R_L, P_R)\n",
    "    \n",
    "    return JS_temp, Gain_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance inflation factor calculation formula\n",
    "def vif(test_data, factor):\n",
    "    cols = list(test_data.columns)\n",
    "    cols.remove(factor)\n",
    "    cols_noti = cols\n",
    "    formula = factor + '~' + '+'.join(cols_noti)\n",
    "    r2 = ols(formula, test_data).fit().rsquared\n",
    "    return 1. / (1. - r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important matrix (SI)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#The point file used to calculate the importance coefficient needs to be prepared in advance. \n",
    "#The point file consists of random points generated in the area plus high risk slopes (HRSs). \n",
    "#Then the values of each environmental geological factor are extracted into the attribute table of the point feature.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Sample for important analysis\n",
    "QinBa_sample = os.path.join(factors_address, 'QinBa_SampleForSA')\n",
    "\n",
    "#Fields of the sample\n",
    "QinBa_sample_desc = arcpy.Describe(QinBa_sample)\n",
    "QinBa_sample_fields = [i.name for i in QinBa_sample_desc.fields]\n",
    "\n",
    "# Features for sensitivity analysis\n",
    "fields_list = QinBa_sample_fields[3:]\n",
    "QinBa_sample_df = CovertFeature2df(QinBa_sample, fields_list)\n",
    "\n",
    "# Dataset labels, identify HRSs set (y = 1) and random set(y = 0) \n",
    "QinBa_sample_label = 'y'\n",
    "QinBa_label_df = CovertFeature2df(QinBa_sample, [QinBa_sample_label])\n",
    "\n",
    "#HRSs dataset and random dataset(all_feature)\n",
    "HRSs_feature_df = QinBa_sample_df.loc[QinBa_label_df[QinBa_sample_label] == 1, :].reset_index()\n",
    "RandomPoint_feature_df = QinBa_sample_df.loc[QinBa_label_df[QinBa_sample_label] == 0, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## important matrix (SI)\n",
    "\n",
    "fields_list = ['TWI', 'SurfaceRoughness', 'SoilType', 'Slope', 'ReliefDegree', 'PlaneCurvature', 'NDVI', 'Map', 'Lithology', 'Landuse', 'FaultDensity', 'DistanceToRoad', 'DistanceToRiver', 'DEM', 'Aspect']\n",
    "continue_list = [True, True, False, True, True, False, True, True, False, False, True, True, True, True, False]\n",
    "\n",
    "JS_list = []\n",
    "Gain_list = []\n",
    "\n",
    "for key, field in enumerate(fields_list):\n",
    "    \n",
    "    temp_landslide = HRSs_feature_df.loc[:, [field]]\n",
    "    temp_No_landsldie = RandomPoint_feature_df.loc[:, [field]]\n",
    "    \n",
    "    temp_JS,  temp_Gain= calculate_important_factor(temp_landslide, temp_No_landsldie, field, bin_x = None, Continue = continue_list[key])\n",
    "    JS_list.append(temp_JS)\n",
    "    Gain_list.append(temp_Gain)\n",
    "    \n",
    "sen_pd = pd.DataFrame()\n",
    "sen_pd['factor_name'] = np.array(fields_list)\n",
    "sen_pd['JS'] = np.array(JS_list)\n",
    "sen_pd['Gain'] = np.array(Gain_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collinearity test for each factor\n",
    "test_data = RandomPoint_feature_df[fields_list]\n",
    "for i in test_data.columns:\n",
    "    print(i, '\\t', vif(test_data, i)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing HRSs and generate non-HRSs, to reassign environmental geological factors and train susceptibility models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing HRSs\n",
    "HRSs_data = os.path.join(factors_address, 'QinBa_HRSs')\n",
    "arcpy.CopyFeatures_management(HRSs_data, os.path.join(factors_address, 'QinBa_HRSs_Copy'))\n",
    "\n",
    "desc = arcpy.Describe(os.path.join(factors_address, 'QinBa_HRSs_Copy'))\n",
    "field_list = [i.name for i in desc.fields]\n",
    "print(field_list)\n",
    "    \n",
    "#Delete redundant fields for HRSs points\n",
    "try:\n",
    "    arcpy.DeleteField_management(os.path.join(factors_address, 'QinBa_HRSs_Copy'), \n",
    "                             field_list[2:], 'DELETE_FIELDS')\n",
    "    print(\"deleted!\")\n",
    "except Exception as e:\n",
    "    print(f\"wrong：{str(e)}\")\n",
    "    \n",
    "#generate non_HRSs\n",
    "#first, calculate the No. of HRSs\n",
    "with arcpy.da.SearchCursor(os.path.join(factors_address, 'QinBa_HRSs_Copy'), '*') as cursor:\n",
    "    row_count = sum(1 for row in cursor)\n",
    "\n",
    "no_landslide = arcpy.CreateRandomPoints_management(factors_address, 'Non_HRSs', \n",
    "                                    os.path.join(factors_address, 'QinBa_Areas'), \"\", row_count, \"\", \n",
    "                                    \"POINT\")\n",
    "desc = arcpy.Describe(os.path.join(factors_address, 'Non_HRSs'))\n",
    "field_list = [i.name for i in desc.fields]\n",
    "print(field_list)\n",
    "    \n",
    "#Delete redundant fields for non_HRSs points\n",
    "try:\n",
    "    arcpy.DeleteField_management(os.path.join(factors_address, 'Non_HRSs'), \n",
    "                             field_list[2:], 'DELETE_FIELDS')\n",
    "    print(\"deleted!\")\n",
    "except Exception as e:\n",
    "    print(f\"wrong：{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label for HRSs and non_HRSs\n",
    "HRSs = os.path.join(factors_address, 'QinBa_HRSs_Copy')\n",
    "Non_HRSs = os.path.join(factors_address, 'Non_HRSs')\n",
    "\n",
    "field_name = 'label'\n",
    "field_type = 'SHORT'\n",
    "\n",
    "try:\n",
    "    # 添加新字段\n",
    "    arcpy.AddField_management(HRSs, field_name, field_type)\n",
    "    arcpy.AddField_management(Non_HRSs, field_name, field_type)\n",
    "\n",
    "    # 为新字段的所有行设置值为1\n",
    "    land_field = arcpy.CalculateField_management(HRSs, field_name, 1, \"PYTHON\")\n",
    "    noland_field = arcpy.CalculateField_management(Non_HRSs, field_name, 0, \"PYTHON\")\n",
    "\n",
    "    print('Successfully added and assigned!')\n",
    "except Exception as e:\n",
    "    print(f\"wrong：{str(e)}\")\n",
    "    \n",
    "# Merge the HRSs and Non_HRSs\n",
    "output_path = os.path.join(factors_address, 'training_data')\n",
    "try:\n",
    "    arcpy.management.Merge(inputs = [HRSs, Non_HRSs], output = output_path)\n",
    "    print(f\"Merge seccessful\")\n",
    "except Exception as e:\n",
    "    print(f\"wrong：{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract factors values to merged training data\n",
    "\n",
    "#List the Geo-environmental factors after collinearity filtering\n",
    "factors_name = ['Factor_QinBa_DEM', 'Factor_QinBa_Slope', 'Factor_QinBa_Aspect', 'Factor_QinBa_PlaneCurvature', 'Factor_QinBa_TWI',\n",
    "               'Factor_QinBa_DistanceToRiver', 'Factor_QinBa_MAP', 'Factor_QinBa_Lithology', 'Factor_QinBa_FaultDensity', 'Factor_QinBa_NDVI',\n",
    "               'Factor_QinBa_SoilType', 'Factor_QinBa_DistanceToRoad', 'Factor_QinBa_Landuse']\n",
    "factors_rename = ['DEM', 'Slope', 'Aspect', 'Curvature', 'TWI', 'River', 'MAP',\n",
    "                 'Lithology', 'Fault', 'NDVI', 'SoilType', 'Road', 'Landuse']\n",
    "\n",
    "sample_point = os.path.join(factors_address, 'training_data')\n",
    "Raster_path_pair = [[os.path.join(factors_address, factors_name[i]), factors_rename[i]] for i in range(len(factors_name))]\n",
    "\n",
    "arcpy.sa.ExtractMultiValuesToPoints(sample_point, Raster_path_pair, \"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the attribute table of training data to Pandas DataFrame\n",
    "\n",
    "sample_point = os.path.join(factors_address, 'training_data')\n",
    "land_field_list = factors_rename.copy()\n",
    "land_field_list = land_field_list + ['OBJECTID', 'label']\n",
    "\n",
    "sample_df = pd.DataFrame(arcpy.da.TableToNumPyArray(sample_point, land_field_list))\n",
    "sample_df.set_index('OBJECTID', drop = True, inplace = True)\n",
    "\n",
    "#Note: It is best not to set the fields in the attribute table to short integers. \n",
    "#If they are short integers, you need to manually fill in the missing values in the short integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_df.iloc[:, :-1], sample_df.iloc[:, [-1]],test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency ratio function\n",
    "def RasterFrequencyRatio(Raster_address,landslide_data, landslide_field, bins_num = 30, bins_input = False, train_ratio = 0.7):\n",
    "    \n",
    "    #Geo-environmental factors' information\n",
    "    Raster_data = Raster(Raster_address)\n",
    "    \n",
    "    Raster_data_maximum = Raster_data.maximum\n",
    "    Raster_data_minmum = Raster_data.minimum\n",
    "    \n",
    "    Raster_data_cell_width = Raster_data.meanCellWidth\n",
    "    Raster_data_cell_height = Raster_data.meanCellHeight\n",
    "    \n",
    "    #Convert raster to NumPyArray and calculate histogram\n",
    "    Raster_array = arcpy.RasterToNumPyArray(Raster_data, nodata_to_value = 999999)\n",
    "    Raster_array = Raster_array.flatten()\n",
    "    \n",
    "    if bins_input:\n",
    "        hist_raster, bins_raster = np.histogram(Raster_array, bins = bins_input)\n",
    "    else:\n",
    "        hist_raster, bins_raster = np.histogram(Raster_array, bins = np.linspace(Raster_data_minmum, Raster_data_maximum+0.1, bins_num))\n",
    "    \n",
    "    #Calculate the percentage of rasters within each histogram\n",
    "    hist_raster_density = hist_raster/hist_raster.sum()\n",
    "    \n",
    "    #Calculate the percentage of HRSs within each histogram\n",
    "    train_set, test_set = train_test_split(landslide_data, test_size = 1-train_ratio, random_state = 42)\n",
    "    hist_land,_ = np.histogram(train_set.loc[:, landslide_field], bins = bins_raster)#landslide_field为所需统计直方的因子名称\n",
    "    hist_land_density = hist_land/hist_land.sum()\n",
    "    \n",
    "    #Calculate FR\n",
    "    hist_raster_density[hist_raster_density == 0] = 0.001\n",
    "    Fr = hist_land_density/hist_raster_density\n",
    "    record = {'start_point': bins_raster[:-1], 'end_point':bins_raster[1:], \n",
    "              'raster_num':hist_raster,'landslide_num':hist_land,\n",
    "              'raster_percentage':hist_raster_density,'landslide_percentage':hist_land_density,'Fr_value':Fr}\n",
    "    record_df = pd.DataFrame(record)\n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency ratio function\n",
    "def RasterFrequencyRatio(Raster_address,landslide_data, landslide_field, bins_num = 30, bins_input = False, train_ratio = 0.7):\n",
    "    \n",
    "    #Geo-environmental factors' information\n",
    "    Raster_data = Raster(Raster_address)\n",
    "    \n",
    "    Raster_data_maximum = Raster_data.maximum\n",
    "    Raster_data_minmum = Raster_data.minimum\n",
    "    \n",
    "    Raster_data_cell_width = Raster_data.meanCellWidth\n",
    "    Raster_data_cell_height = Raster_data.meanCellHeight\n",
    "    \n",
    "    #Convert raster to NumPyArray and calculate histogram\n",
    "    Raster_array = arcpy.RasterToNumPyArray(Raster_data, nodata_to_value = 999999)\n",
    "    Raster_array = Raster_array.flatten()\n",
    "    \n",
    "    if bins_input:\n",
    "        hist_raster, bins_raster = np.histogram(Raster_array, bins = bins_input)\n",
    "    else:\n",
    "        hist_raster, bins_raster = np.histogram(Raster_array, bins = np.linspace(Raster_data_minmum, Raster_data_maximum+0.1, bins_num))\n",
    "    \n",
    "    #Calculate the percentage of rasters within each histogram\n",
    "    hist_raster_density = hist_raster/hist_raster.sum()\n",
    "    \n",
    "    #Calculate the percentage of HRSs within each histogram\n",
    "    train_set, test_set = train_test_split(landslide_data, test_size = 1-train_ratio, random_state = 42)\n",
    "    hist_land,_ = np.histogram(train_set.loc[:, landslide_field], bins = bins_raster)#landslide_field为所需统计直方的因子名称\n",
    "    hist_land_density = hist_land/hist_land.sum()\n",
    "    \n",
    "    #Calculate FR\n",
    "    hist_raster_density[hist_raster_density == 0] = 0.001\n",
    "    Fr = hist_land_density/hist_raster_density\n",
    "    record = {'start_point': bins_raster[:-1], 'end_point':bins_raster[1:], \n",
    "              'raster_num':hist_raster,'landslide_num':hist_land,\n",
    "              'raster_percentage':hist_raster_density,'landslide_percentage':hist_land_density,'Fr_value':Fr}\n",
    "    record_df = pd.DataFrame(record)\n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define another function to reassign the factors\n",
    "def reclass_raster(Raster_address, reclass_statistics_table):\n",
    "    Raster_Raster = Raster(Raster_address)\n",
    "    rmap = []\n",
    "    for index in reclass_statistics_table.index:\n",
    "        temp_list = []\n",
    "        temp_list = reclass_statistics_table.loc[index, ['start_point', 'end_point']].to_list()\n",
    "        temp_list.append(int(reclass_statistics_table.loc[index, 'Fr_value']*10000))\n",
    "        rmap.append(temp_list)\n",
    "    remap = RemapRange(rmap)\n",
    "    Raster_reclass_temp = Reclassify(Raster_Raster, 'VALUE', remap)/10000.0\n",
    "    Raster_reclass_temp.save(str(Raster_address)+'_rereclass')\n",
    "    print(str(Raster_address)+'_rereclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign continuous factors\n",
    "\n",
    "continuous_factors = ['Factor_QinBa_DEM', 'Factor_QinBa_Slope', 'Factor_QinBa_TWI', 'Factor_QinBa_DistanceToRiver',\n",
    "                      'Factor_QinBa_MAP', 'Factor_QinBa_FaultDensity', 'Factor_QinBa_NDVI', 'Factor_QinBa_DistanceToRoad']\n",
    "\n",
    "continuous_factors_rename = ['DEM', 'Slope', 'TWI', 'River', 'MAP', 'Fault', 'NDVI', 'Road']\n",
    "\n",
    "\n",
    "for raster_index in range(len(continuous_factors)):\n",
    "    Raster_path = os.path.join(factors_address, continuous_factors[raster_index])\n",
    "    HRSs_df = X_train[y_train.label == 1]\n",
    "    field_name = continuous_factors_rename[raster_index]\n",
    "\n",
    "    reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_num = 30)\n",
    "    reclass_table.to_excel(os.path.join(tabel_address, continuous_factors[raster_index] +'_1FR.xlsx'))\n",
    "    reclass_raster(Raster_path, reclass_table) #这是第二函数，用来进行重赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency ratio of the rock group and reassign it\n",
    "\n",
    "bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]\n",
    "factors_name = 'Factor_QinBa_Lithology'\n",
    "Raster_path = os.path.join(factors_address, factors_name)\n",
    "field_name = 'Lithology'\n",
    "\n",
    "reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_input = bins)\n",
    "reclass_table.to_excel(os.path.join(tabel_address, factors_name +'_FR.xlsx'))\n",
    "reclass_raster(Raster_path, reclass_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency ratio of the Aspect and reassign it\n",
    "bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5]\n",
    "factors_name = 'Factor_QinBa_Aspect'\n",
    "Raster_path = os.path.join(factors_address, factors_name)\n",
    "field_name = 'Aspect'\n",
    "\n",
    "reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_input = bins)\n",
    "reclass_table.to_excel(os.path.join(tabel_address, factors_name +'_FR.xlsx'))\n",
    "reclass_raster(Raster_path, reclass_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency ratio of the PlaneCurvature and reassign it\n",
    "bins = [0.5, 1.5, 2.5, 3.5]\n",
    "factors_name = 'Factor_QinBa_PlaneCurvature'\n",
    "Raster_path = os.path.join(factors_address, factors_name)\n",
    "field_name = 'Curvature'\n",
    "\n",
    "reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_input = bins)\n",
    "reclass_table.to_excel(os.path.join(tabel_address, factors_name +'_FR.xlsx'))\n",
    "reclass_raster(Raster_path, reclass_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency ratio of the SoilType and reassign it\n",
    "bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]\n",
    "factors_name = 'Factor_QinBa_Landuse'\n",
    "Raster_path = os.path.join(factors_address, factors_name)\n",
    "field_name = 'SoilType'\n",
    "\n",
    "reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_input = bins)\n",
    "reclass_table.to_excel(os.path.join(tabel_address, factors_name +'_FR.xlsx'))\n",
    "reclass_raster(Raster_path, reclass_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency ratio of the Landuse and reassign it\n",
    "bins = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]\n",
    "factors_name = 'Factor_QinBa_Landuse'\n",
    "Raster_path = os.path.join(factors_address, factors_name)\n",
    "field_name = 'Landuse'\n",
    "\n",
    "reclass_table = RasterFrequencyRatio(Raster_path, HRSs_df, field_name, bins_input = bins)\n",
    "reclass_table.to_excel(os.path.join(tabel_address, factors_name +'_FR.xlsx'))\n",
    "reclass_raster(Raster_path, reclass_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landslide susceptibility training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = arcpy.Describe(os.path.join(factors_address, 'training_data'))\n",
    "field_list = [i.name for i in desc.fields]\n",
    "print(field_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the reassigned factor values into sample points\n",
    "\n",
    "#delete the extra fields\n",
    "try:\n",
    "    arcpy.DeleteField_management(os.path.join(factors_address, 'training_data'), \n",
    "                             field_list[3:], 'DELETE_FIELDS')\n",
    "    print(\"deleted!\")\n",
    "except Exception as e:\n",
    "    print(f\"wrong：{str(e)}\")\n",
    "    \n",
    "factors_name = ['Factor_QinBa_DEM', 'Factor_QinBa_Slope', 'Factor_QinBa_Aspect', 'Factor_QinBa_PlaneCurvature', 'Factor_QinBa_TWI',\n",
    "               'Factor_QinBa_DistanceToRiver', 'Factor_QinBa_MAP', 'Factor_QinBa_Lithology', 'Factor_QinBa_FaultDensity', 'Factor_QinBa_NDVI',\n",
    "               'Factor_QinBa_SoilType', 'Factor_QinBa_DistanceToRoad', 'Factor_QinBa_Landuse']\n",
    "factors_ressign_name = [i+'_rereclass' for i in factors_name]\n",
    "fields_rename = ['DEM', 'Slope', 'Aspect', 'Curvature', 'TWI', 'River', 'MAP',\n",
    "                 'Lithology', 'Fault', 'NDVI', 'SoilType', 'Road', 'Landuse']\n",
    "\n",
    "Raster_path_pair = [[os.path.join(factors_address, factors_ressign_name[i]), fields_rename[i]] for i in range(len(factors_name))]\n",
    "\n",
    "arcpy.sa.ExtractMultiValuesToPoints(os.path.join(factors_address, 'training_data'), Raster_path_pair, \"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the 'training_datas' to DataFrame\n",
    "sample_data_fields = arcpy.ListFields(os.path.join(factors_address, 'training_data'))\n",
    "fields_list = [i.name for i in sample_data_fields]\n",
    "sample_data = arcpy.da.TableToNumPyArray(os.path.join(factors_address, 'training_data'), fields_list[2:])\n",
    "sample_data_df = pd.DataFrame(sample_data)\n",
    "sample_data_df = sample_data_df.fillna(0)\n",
    "\n",
    "#stratified sampling\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size = 0.3, random_state= 42)\n",
    "for train_index, test_index in split.split(sample_data_df, sample_data_df.loc[:, 'label']):\n",
    "    train_set = sample_data_df.loc[train_index]\n",
    "    test_set = sample_data_df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "classifier_LR = make_pipeline(StandardScaler(), LogisticRegression(random_state = 0))\n",
    "\n",
    "param_grid = {'logisticregression__C': np.logspace(-1, 1, 10)}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier_LR, param_grid = param_grid,\n",
    "                          cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "with ignore_warnings(category= ConvergenceWarning):\n",
    "    grid_search.fit(train_set.iloc[:, 1:], train_set.iloc[:, 0])\n",
    "    \n",
    "print(\"best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Use the model with the best parameters to make predictions\n",
    "best_lr_classifier = grid_search.best_estimator_\n",
    "y_pred_prob = best_lr_classifier.predict_proba(test_set.iloc[:, 1:])[:, 1]\n",
    "y_pred = best_lr_classifier.predict(test_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(test_set.iloc[:, 0], y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Calculate AUC\n",
    "auc = roc_auc_score(test_set.iloc[:, 0], y_pred_prob)\n",
    "print(\"AUC of Validation set: {:.2f}%\".format(auc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA\n",
    "classifier_DA = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis())\n",
    "with ignore_warnings(category= ConvergenceWarning):\n",
    "    classifier_DA.fit(train_set.iloc[:, 1:], train_set.iloc[:, 0])\n",
    "    \n",
    "\n",
    "y_pred_prob = classifier_DA.predict_proba(test_set.iloc[:, 1:])[:, 1]\n",
    "y_pred = classifier_DA.predict(test_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(test_set.iloc[:, 0], y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Calculate AUC\n",
    "auc = roc_auc_score(test_set.iloc[:, 0], y_pred_prob)\n",
    "print(\"AUC of Validation set: {:.2f}%\".format(auc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "classifier_svc = make_pipeline(StandardScaler(), SVC(random_state = 0, probability = True))\n",
    "GridSearch_svc = {'svc__C': [-1, 1, 10],'svc__gamma':[0.01, 0.25, 0.5, 1] }\n",
    "grid_search = GridSearchCV(estimator = classifier_svc, param_grid = GridSearch_svc, cv = 5)\n",
    "with ignore_warnings(category= ConvergenceWarning):\n",
    "    grid_search.fit(train_set.iloc[:, 1:], train_set.iloc[:, 0])\n",
    "    \n",
    "print(\"best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Use the model with the best parameters to make predictions\n",
    "best_svm_classifier = grid_search.best_estimator_\n",
    "y_pred_prob = best_svm_classifier.predict_proba(test_set.iloc[:, 1:])[:, 1]\n",
    "y_pred = best_svm_classifier.predict(test_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(test_set.iloc[:, 0], y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Calculate AUC\n",
    "auc = roc_auc_score(test_set.iloc[:, 0], y_pred_prob)\n",
    "print(\"AUC of Validation set: {:.2f}%\".format(auc*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "\n",
    "GridSearch_mlp = {'mlpclassifier__hidden_layer_sizes':[(15,), (20,), (25,), (30)],\n",
    "             'mlpclassifier__activation': ['tanh', 'relu'],\n",
    "             'mlpclassifier__alpha':[0.0001, 0.001, 0.01, 0.1],\n",
    "             'mlpclassifier__learning_rate_init':[0.001, 0.01, 0.1]}\n",
    "\n",
    "\n",
    "classifier_mlp = make_pipeline(StandardScaler(), MLPClassifier(solver = 'sgd', random_state = 42))\n",
    "\n",
    "grid_search = GridSearchCV(estimator = classifier_mlp, param_grid = GridSearch_mlp, cv=5 )\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    grid_search.fit(train_set.iloc[:, 1:], train_set.iloc[:, 0])\n",
    "    \n",
    "print('best parameters:', grid_search.best_params_)\n",
    "\n",
    "\n",
    "# Use the model with the best parameters to make predictions\n",
    "y_pred_prob = grid_search.predict_proba(test_set.iloc[:, 1:])[:, 1]\n",
    "y_pred = grid_search.predict(test_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(test_set.iloc[:, 0], y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Calculate AUC\n",
    "auc = roc_auc_score(test_set.iloc[:, 0], y_pred_prob)\n",
    "print(\"AUC of Validation set: {:.2f}%\".format(auc*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_search.fit(train_set.iloc[:, 1:], train_set.iloc[:, 0])\n",
    "\n",
    "print(\"best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Use the model with the best parameters to make predictions\n",
    "best_RFC_classifier = grid_search.best_estimator_\n",
    "y_pred_prob = best_RFC_classifier.predict_proba(test_set.iloc[:, 1:])[:, 1]\n",
    "y_pred = best_RFC_classifier.predict(test_set.iloc[:, 1:])\n",
    "\n",
    "# Calculate model accuracy\n",
    "accuracy = accuracy_score(test_set.iloc[:, 0], y_pred)\n",
    "print(\"Validation set accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#Calculate AUC\n",
    "auc = roc_auc_score(test_set.iloc[:, 0], y_pred_prob)\n",
    "print(\"AUC of Validation set: {:.2f}%\".format(auc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the raster of the study area (30m by 30m) into a point file, \n",
    "#extract the factor values into the points, and use four classifiers \n",
    "#to calculate the susceptibility index of each point separately\n",
    "\n",
    "#Four optimal machine learning classifiers were obtained, namely：\n",
    "best_lr_classifier\n",
    "best_svm_classifier\n",
    "classifier_mlp\n",
    "best_RFC_classifier\n",
    "\n",
    "# Convert raster to point features\n",
    "Raster_point = os.path.join(factors_address, 'Raster_point')\n",
    "arcpy.conversion.RasterToPoint('Factor_QinBa_DEM', os.path.join(factors_address, Raster_point), 'Value')\n",
    "\n",
    "arcpy.sa.ExtractMultiValuesToPoints(Raster_point, Raster_path_pair, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature point attribute table to pd.DataFrame\n",
    "field_list = arcpy.ListFields(Raster_point)\n",
    "field_list = [i.name for i in field_list]\n",
    "sus_calculate_array = arcpy.da.TableToNumPyArray(Raster_point,[field_list[0]]+field_list[4:])\n",
    "sus_calculate_df = pd.DataFrame(sus_calculate_array, columns = [field_list[0]]+field_list[4:])\n",
    "\n",
    "# Fill in NA values\n",
    "# The filling method is to fill in the nearest neighboring value\n",
    "sus_calculate_df_withoutNA = sus_calculate_df.fillna(method=\"ffill\")\n",
    "#The NA value in the first row is filled in with 0\n",
    "sus_calculate_df_withoutNA = sus_calculate_df_withoutNA.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute susceptibility index of grid points using different models\n",
    "\n",
    "#LR\n",
    "prob_LR0_3k = best_lr_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[:30000000,1:])[:,1]\n",
    "prob_LR3_6k = best_lr_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[30000000:60000000,1:])[:,1]\n",
    "prob_LR6_9k = best_lr_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[60000000:,1:])[:,1]\n",
    "\n",
    "prob_LR = np.concatenate((prob_LR0_3k, prob_LR3_6k, prob_LR6_9k))\n",
    "\n",
    "#SVM\n",
    "# Takes about 5 hours\n",
    "prob_SVM0_3k = best_svm_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[:30000000,1:])[:,1]\n",
    "prob_SVM3_6k = best_svm_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[30000000:60000000,1:])[:,1]\n",
    "prob_SVM6_9k = best_svm_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[60000000:,1:])[:,1]\n",
    "\n",
    "prob_SVM = np.concatenate((prob_SVM0_3k, prob_SVM3_6k, prob_SVM6_9k))\n",
    "\n",
    "#MLP\n",
    "prob_MLP0_3k = classifier_mlp.predict_proba(sus_calculate_df_withoutNA.iloc[:30000000,1:])[:,1]\n",
    "prob_MLP3_6k = classifier_mlp.predict_proba(sus_calculate_df_withoutNA.iloc[30000000:60000000,1:])[:,1]\n",
    "prob_MLP6_9k = classifier_mlp.predict_proba(sus_calculate_df_withoutNA.iloc[60000000:,1:])[:,1]\n",
    "\n",
    "prob_MLP = np.concatenate((prob_MLP0_3k, prob_MLP3_6k, prob_MLP6_9k))\n",
    "\n",
    "#RF\n",
    "# take about 5 minutes\n",
    "prob_RFC0_3k = best_RFC_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[:30000000,1:])[:,1]\n",
    "prob_RFC3_6k = best_RFC_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[30000000:60000000,1:])[:,1]\n",
    "prob_RFC6_9k = best_RFC_classifier.predict_proba(sus_calculate_df_withoutNA.iloc[60000000:,1:])[:,1]\n",
    "\n",
    "prob_RFC = np.concatenate((prob_RFC0_3k, prob_RFC3_6k, prob_RFC6_9k))\n",
    "\n",
    "sus_calculate_df_withoutNA['prob_LR'] = prob_LR\n",
    "sus_calculate_df_withoutNA['prob_SVM'] = prob_SVM\n",
    "sus_calculate_df_withoutNA['prob_MLP'] = prob_MLP\n",
    "sus_calculate_df_withoutNA['prob_RFC'] = prob_RFC\n",
    "\n",
    "#Extract the latitude and longitude coordinates of each raster point\n",
    "sus_calculate_array = arcpy.da.TableToNumPyArray(Raster_point,['SHAPE@X','SHAPE@Y']+[field_list[0]])\n",
    "sus_calculate_df = pd.DataFrame(sus_calculate_array)\n",
    "data = sus_calculate_df.merge(sus_calculate_df_withoutNA, left_on = 'OBJECTID', right_on = 'OBJECTID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gdal to generate susceptibility distribution plots\n",
    "def conduct_LSM(data_array, Raster_model, save_path, file_name):\n",
    "    #Identify longitude and latitude grid coordinates\n",
    "    var_lon = data_array.columns.map(float)\n",
    "    var_lat = data_array.index\n",
    "    \n",
    "    #Identify the susceptibility value of each grid point\n",
    "    var_value = data_array.values\n",
    "    #data_arr = np.asarray(gdal_array)\n",
    "    \n",
    "    #Identify the scope of LSM\n",
    "    LonMin, LatMax, LonMax, LatMin = [var_lon.min(), var_lat.max(), var_lon.max(), var_lat.min()]\n",
    "    N_Lat = len(var_lat)\n",
    "    N_Lon = len(var_lon)\n",
    "\n",
    "    #Calculate raster spatial resolution\n",
    "    Lon_Res = (var_lon[1:].values - var_lon[:-1].values)[0]\n",
    "    Lat_Res = (var_lat[1:].values - var_lat[:-1].values)[0]\n",
    "    \n",
    "    Lon_Res = abs(Lon_Res)\n",
    "    Lat_Res = abs(Lat_Res)\n",
    "    \n",
    "    # Create a grid driver\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    #Create raster\n",
    "    out_tif = driver.Create(os.path.join(save_path, file_name), N_Lon, N_Lat, 1, gdal.GDT_Float32)\n",
    "    \n",
    "    # Set projection and conversion information for an image\n",
    "    \n",
    "    #Get projection and transformation information\n",
    "    GeoTransform = (LonMin, Lon_Res, 0, LatMax, 0, -Lat_Res)\n",
    "    GeoProjection = gdal.Open(Raster_model).GetProjection()\n",
    "    #\n",
    "    out_tif.SetGeoTransform(GeoTransform)\n",
    "    out_tif.SetProjection(GeoProjection)\n",
    "    \n",
    "    # Data writing\n",
    "    out_tif.GetRasterBand(1).WriteArray(var_value)  # 将数据写入内存，此时没有写入硬盘\n",
    "    out_tif.FlushCache()  # 将数据写入硬盘\n",
    "    out_tif = None  # 注意必须关闭tif文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LSM\n",
    "gdal_array_MLP = data.pivot(index = 'SHAPE@Y', columns = 'SHAPE@X', values='prob_MLP')\n",
    "gdal_array_LR = data.pivot(index = 'SHAPE@Y', columns = 'SHAPE@X', values='prob_LR')\n",
    "gdal_array_RFC = data.pivot(index = 'SHAPE@Y', columns = 'SHAPE@X', values='prob_RFC')\n",
    "gdal_array_SVM = data.pivot(index = 'SHAPE@Y', columns = 'SHAPE@X', values='prob_SVM')\n",
    "\n",
    "file_path = r'E:\\LSM'\n",
    "raster_model = 'DEM.tif'\n",
    "file_name_LR = 'LSM_LR.tif'\n",
    "file_name_MLP = 'LSM_MLP.tif'\n",
    "file_name_RFC = 'LSM_RFC.tif'\n",
    "file_name_SVM = 'LSM_SVM.tif'\n",
    "conduct_LSM(gdal_array_MLP.iloc[::-1], os.path.join(file_path, raster_model), file_path, file_name_MLP)\n",
    "conduct_LSM(gdal_array_LR.iloc[::-1], os.path.join(file_path, raster_model), file_path, file_name_LR)\n",
    "conduct_LSM(gdal_array_RFC.iloc[::-1], os.path.join(file_path, raster_model), file_path, file_name_RFC)\n",
    "conduct_LSM(gdal_array_SVM.iloc[::-1], os.path.join(file_path, raster_model), file_path, file_name_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calculation functions for ROC, AUC, Acc, Precision, Recall, F-measure and other indicators\n",
    "def calculate_common_indicators(feature_class, prob_field, label, bins = np.linspace(0, 1, 50)):\n",
    "    # feature class is a combined data set of landslide and non-landslide points, which is used to test the performance of LSM\n",
    "    # prob_field is the field name of the landslide susceptibility index in the data set\n",
    "    # label indicates whether the columns is a landslide or a non-landslide.\n",
    "    \n",
    "    feature_df = arcpy.da.TableToNumPyArray(feature_class, [prob_field, label])\n",
    "    feature_df = pd.DataFrame(feature_df)\n",
    "    feature_df.fillna(0.5, inplace = True)\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    for index in bins:\n",
    "        temp_tp = feature_df.loc[(feature_df.loc[:, prob_field] >= index) & (feature_df.loc[:, label] == 1)].shape[0]\n",
    "        temp_fp = feature_df.loc[(feature_df.loc[:, prob_field] >= index) & (feature_df.loc[:, label] == 0)].shape[0]\n",
    "        temp_fn = feature_df.loc[(feature_df.loc[:, prob_field] <  index) & (feature_df.loc[:, label] == 1)].shape[0]\n",
    "        temp_tn = feature_df.loc[(feature_df.loc[:, prob_field] <  index) & (feature_df.loc[:, label] == 0)].shape[0]\n",
    "        \n",
    "        temp_TPR = temp_tp/(temp_tp+temp_fn)\n",
    "        temp_FPR = temp_fp/(temp_fp+temp_tn)\n",
    "        \n",
    "        tpr_list.append(temp_TPR)\n",
    "        fpr_list.append(temp_FPR)\n",
    "        \n",
    "    index = 0.5\n",
    "    tp = feature_df.loc[(feature_df.loc[:, prob_field] >= index) & (feature_df.loc[:, label] == 1)].shape[0]\n",
    "    fp = feature_df.loc[(feature_df.loc[:, prob_field] >= index) & (feature_df.loc[:, label] == 0)].shape[0]\n",
    "    fn = feature_df.loc[(feature_df.loc[:, prob_field] <  index) & (feature_df.loc[:, label] == 1)].shape[0]\n",
    "    tn = feature_df.loc[(feature_df.loc[:, prob_field] <  index) & (feature_df.loc[:, label] == 0)].shape[0]\n",
    "    \n",
    "    \n",
    "    ACC = (tn + tp)/(tn + tp + fn + fp)\n",
    "    Precision = tp/(tp + fp)\n",
    "    Recall = tp/(tp + fn)\n",
    "    F_measure = (2*tp)/(2*tp + fn + fp)\n",
    "    \n",
    "    ROC = pd.DataFrame()    \n",
    "    ROC[prob_field+ '_FPR'] = fpr_list\n",
    "    ROC[prob_field+ '_TPR'] = tpr_list\n",
    "    \n",
    "    \n",
    "    bins_width = np.array(fpr_list[:-1]) - np.array(fpr_list[1:])\n",
    "    tpr_hight = np.array(tpr_list[:-1]) + (np.array(tpr_list[1:]) - np.array(tpr_list[:-1]))/2\n",
    "    AUC = sum(bins_width*tpr_hight)\n",
    "    return AUC, ROC, ACC, Precision, Recall, F_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction rate curve calculation function\n",
    "# The number of grid cells in each bin is equal\n",
    "def Calculate_PredicCurve(landslide_class, field_name, LSM_raster, bins = 30):\n",
    "    \n",
    "    temp_raster = Raster(LSM_raster)\n",
    "    \n",
    "    #Calculate the maximum and minimum values of a raster\n",
    "    max_boundary = temp_raster.maximum\n",
    "    min_boundary = temp_raster.minimum\n",
    "    \n",
    "    # Convert raster to numpy array\n",
    "    arr = arcpy.RasterToNumPyArray(temp_raster,nodata_to_value=-99999)\n",
    "    \n",
    "    # Flatten and sort numpy array\n",
    "    arr = arr.flatten()\n",
    "    arr = np.sort(arr)\n",
    "    \n",
    "    # Get valid values (remove the value of Nodata)\n",
    "    arr = arr[arr>= min_boundary]\n",
    "    \n",
    "    # Count the number of rasters within a bin cell\n",
    "    raster_num = len(arr)\n",
    "    bin_width = raster_num//bins + 1\n",
    "    bins_edge = bin_width*np.arange(0, bins+1)\n",
    "    \n",
    "    # Calculate the boundary value of bin\n",
    "    bins_edge = [arr[i] for i in bins_edge[:-1]]\n",
    "    bins_edge.append(max_boundary)\n",
    "    \n",
    "    bins_edge[1:] = [values + 0.00000001*(key+1) for key, values in enumerate(bins_edge[1:])]\n",
    "    \n",
    "    Raster_num = pd.cut(pd.Series(arr), bins_edge, right = False).value_counts(normalize=True,sort = False)\n",
    "    \n",
    "    # Count the number of landslide points in each bin \n",
    "    feature_df = arcpy.da.TableToNumPyArray(landslide_class, [field_name])\n",
    "    feature_df = pd.DataFrame(feature_df)\n",
    "    feature_df.fillna(0.5, inplace = True)\n",
    "    landslide_num = pd.cut(feature_df[field_name], bins_edge).value_counts(sort = False).values\n",
    "    \n",
    "    #Output results\n",
    "    statistic_infor = pd.DataFrame(Raster_num.values*100, index = Raster_num.index, columns = ['Raster_num / %'])\n",
    "    \n",
    "    statistic_infor['LSI'] = (np.array(bins_edge[:-1]) + np.array(bins_edge[1:]))/2\n",
    "    statistic_infor['landslide_num'] = landslide_num\n",
    "    statistic_infor['landslide_perc'] = landslide_num/landslide_num.sum()*100\n",
    "    statistic_infor['landslide_cum_perc'] = landslide_num.cumsum()/landslide_num.sum()\n",
    "    statistic_infor['landslide_cumsum'] = landslide_num.cumsum()\n",
    "    statistic_infor['FR'] = statistic_infor['landslide_perc']/statistic_infor['Raster_num / %']\n",
    "    \n",
    "    \n",
    "    # Calculate the area under the prediction rate curve, which is the indicator PAS\n",
    "    area_perc = landslide_num[::-1]*np.linspace(1, 0, bins)\n",
    "    PAS = area_perc.sum()/landslide_num.sum()\n",
    "    \n",
    "    return PAS, statistic_infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSM sensitivity index(SI) calculation function\n",
    "def FrequencyRatio_Curve(landslide_class, field_name, LSM_raster, bins = 30):\n",
    "    \n",
    "    temp_raster = Raster(LSM_raster)\n",
    "    \n",
    "    max_boundary = temp_raster.maximum\n",
    "    min_boundary = temp_raster.minimum\n",
    "    \n",
    "    arr = arcpy.RasterToNumPyArray(temp_raster,nodata_to_value=-99999)\n",
    "    \n",
    "    arr = arr.flatten()\n",
    "    arr = np.sort(arr)\n",
    "    \n",
    "    arr = arr[arr>= min_boundary]\n",
    "        \n",
    "    bins_edge = np.linspace(0, 1.0001, bins)\n",
    "    \n",
    "    Raster_num_norm = pd.cut(pd.Series(arr), bins_edge, right = False).value_counts(normalize=True,sort = False)\n",
    "    Raster_num = pd.cut(pd.Series(arr), bins_edge, right = False).value_counts(sort = False)\n",
    "    \n",
    "    feature_df = arcpy.da.TableToNumPyArray(landslide_class, [field_name])\n",
    "    feature_df = pd.DataFrame(feature_df)\n",
    "    feature_df.fillna(0.5, inplace = True)\n",
    "    landslide_num = pd.cut(feature_df[field_name], bins_edge).value_counts(sort = False).values\n",
    "    \n",
    "    \n",
    "    # Calculate sensitivity coefficient\n",
    "    h_full = Raster_num.values\n",
    "    h_land = landslide_num\n",
    "    \n",
    "    h_full[h_full == 0] = 100 # avoid 0/0\n",
    "    h_land[h_land == 0] = 1\n",
    "    \n",
    "    # Parts of Bayesian Formula\n",
    "    P_R = h_full/h_full.sum()\n",
    "    P_L = h_land.sum()/h_full.sum()\n",
    "    P_R_L = h_land/h_land.sum()\n",
    "    P_L_R = h_land/h_full\n",
    "    P_L_R[P_L_R > 1] = 1\n",
    "    bin_width = 1/len(bins_edge[0:-1])\n",
    "    \n",
    "    # Calculate SI\n",
    "    Ent_func = lambda Px, Py: -(Px/(Px+Py)*np.log2(Px/(Px+Py))+Py/(Px+Py)*np.log2(Py/(Px+Py)))\n",
    "    Vector_Ent_func = np.vectorize(Ent_func)\n",
    "    \n",
    "    Ent_feature = Vector_Ent_func(h_land, h_full)  \n",
    "    Gain_temp = Ent_func(h_land.sum(), h_full.sum())- np.sum((h_land+h_full)/np.sum(h_land+h_full) * Ent_feature)\n",
    "    \n",
    "    JS_divergence_func = lambda x, y: 0.5*np.sum(x*np.log2(2*x/(x+y)))+0.5*np.sum(y*np.log2(2*y/(x+y)))\n",
    "    P_R_L[P_R_L == 0]=1e-8 \n",
    "    JS_temp = JS_divergence_func(P_R_L, P_R)\n",
    "    \n",
    "    \n",
    "    #output results\n",
    "    statistic_infor = pd.DataFrame(Raster_num_norm.values*100, index = Raster_num_norm.index, columns = ['Raster_num / %'])\n",
    "    \n",
    "    statistic_infor['LSI'] = (np.array(bins_edge[:-1]) + np.array(bins_edge[1:]))/2\n",
    "    statistic_infor['landslide_num'] = landslide_num\n",
    "    statistic_infor['landslide_perc'] = landslide_num/landslide_num.sum()*100\n",
    "    statistic_infor['landslide_cum_perc'] = landslide_num.cumsum()/landslide_num.sum()\n",
    "    statistic_infor['landslide_cumsum'] = landslide_num.cumsum()\n",
    "    statistic_infor['FR'] = statistic_infor['landslide_perc']/statistic_infor['Raster_num / %']\n",
    "    \n",
    "    return JS_temp, Gain_temp, statistic_infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the value of LSM to the landslide point\n",
    "landslides_data = 'landslides_data'\n",
    "Merged_data = 'performance_evaluated_data'\n",
    "\n",
    "LSM_results = ['LSM_AHP', 'LSM_LR', 'LSM_DA', 'LSM_SVM', 'LSM_MLP', 'LSM_RFC', 'LSM_AlexNet', 'LSM_CNN1D', 'LSM_CNN2D', 'LSM_LSTM']\n",
    "\n",
    "fields_rename = ['AHP', 'LR', 'DA','SVM', 'MLP', 'RFC', 'AlexNet', 'CNN1D', 'CNN2D', 'LSTM']\n",
    "\n",
    "Raster_field_pair = [[os.path.join(factors_address, LSM_results[i]), fields_rename[i]] for i in np.arange(0,len(LSM_results))]\n",
    "arcpy.sa.ExtractMultiValuesToPoints(landslides_data, Raster_field_pair, 'NONE')\n",
    "arcpy.sa.ExtractMultiValuesToPoints(Merged_data, Raster_field_pair, 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC, AUC, ACC and other indices\n",
    "\n",
    "data_fields = arcpy.ListFields(Merged_data)\n",
    "fields_list = [i.name for i in data_fields]\n",
    "fields_list = fields_list[3:]\n",
    "print(fields_list)\n",
    "\n",
    "label = 'label'\n",
    "AUC_list = []\n",
    "ACC_list = []\n",
    "Precision_list = []\n",
    "Recall_list = []\n",
    "F_list = [] \n",
    "ROC_df = pd.DataFrame()\n",
    "\n",
    "for field in fields_list:\n",
    "    AUC, ROC, ACC, Precision, Recall, F_measure= calculate_ROC(Merged_data, field, label)\n",
    "    \n",
    "    AUC_list.append(AUC)\n",
    "    ACC_list.append(ACC)\n",
    "    Precision_list.append(Precision)\n",
    "    Recall_list.append(Recall)\n",
    "    F_list.append(F_measure)\n",
    "    \n",
    "    ROC_df = pd.concat([ROC_df, ROC], axis = 1)\n",
    "    \n",
    "matrixs_df = pd.DataFrame()\n",
    "matrixs_df['field'] = fields_list\n",
    "matrixs_df['AUC'] = AUC_list\n",
    "matrixs_df['ACC'] = ACC_list\n",
    "matrixs_df['Precision'] = Precision_list\n",
    "matrixs_df['Recall'] = Recall_list\n",
    "matrixs_df['F_measure'] = F_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate PAS\n",
    "\n",
    "data_fields = arcpy.ListFields(landslides_data)\n",
    "fields_list = [i.name for i in data_fields]\n",
    "fields_list = fields_list[3:]\n",
    "\n",
    "area_under_PredicCurve = []\n",
    "area_under_PredicCurve_df = pd.DataFrame()\n",
    "#for field in fields_list[-1]:\n",
    "for field in Raster_field_pair:\n",
    "    print('Start calculating the prediction rate curve of {}'.format(field[1]))\n",
    "    area, statistic_infor = Calculate_PredicCurve(landslides_data, field[1], field[0], bins = 30)\n",
    "    area_under_PredicCurve.append(area)\n",
    "    statistic_infor.to_excel(os.path.join(table_address, field[1]+'.xlsx'))\n",
    "    print('completed')\n",
    "area_under_PredicCurve_df['LSM_type'] = fields_list\n",
    "area_under_PredicCurve_df['Area'] = area_under_PredicCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate SI\n",
    "\n",
    "data_fields = arcpy.ListFields(landslides_data)\n",
    "fields_list = [i.name for i in data_fields]\n",
    "fields_list = fields_list[3:]\n",
    "\n",
    "SI_JS = []\n",
    "SI_Gain = []\n",
    "SI_df = pd.DataFrame()\n",
    "#for field in fields_list[-1]:\n",
    "for field in Raster_field_pair:\n",
    "    print('Start calculating the frequence of {}'.format(field[1]))\n",
    "    temp_JS, temp_Gain, statistic_infor = FrequencyRatio_Curve(landslides_data, field[1], field[0], bins = 30)\n",
    "    SI_JS.append(temp_JS)\n",
    "    SI_Gain.append(temp_Gain)\n",
    "    #statistic_infor.to_excel(os.path.join(table_address, field[1]+'_FR.xlsx'))\n",
    "    print('completed')\n",
    "    \n",
    "SI_df['LSM_type'] = fields_list\n",
    "SI_df['JS'] = SI_JS\n",
    "SI_df['Gain'] = SI_Gain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
